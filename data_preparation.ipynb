{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (70000, 37)\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "                                                text       id  \\\n",
      "0                                    That game hurt.  eew5j0j   \n",
      "1   >sexuality shouldn‚Äôt be a grouping category I...  eemcysk   \n",
      "2     You do right, if you don't care then fuck 'em!  ed2mah1   \n",
      "3                                 Man I love reddit.  eeibobj   \n",
      "4  [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
      "\n",
      "                author            subreddit    link_id   parent_id  \\\n",
      "0                Brdd9                  nrl  t3_ajis4z  t1_eew18eq   \n",
      "1          TheGreen888     unpopularopinion  t3_ai4q37   t3_ai4q37   \n",
      "2             Labalool          confessions  t3_abru74  t1_ed2m7g7   \n",
      "3        MrsRobertshaw             facepalm  t3_ahulml   t3_ahulml   \n",
      "4  American_Fascist713  starwarsspeculation  t3_ackt2f  t1_eda65q2   \n",
      "\n",
      "    created_utc  rater_id  example_very_unclear  admiration  ...  love  \\\n",
      "0  1.548381e+09         1                 False           0  ...     0   \n",
      "1  1.548084e+09        37                  True           0  ...     0   \n",
      "2  1.546428e+09        37                 False           0  ...     0   \n",
      "3  1.547965e+09        18                 False           0  ...     1   \n",
      "4  1.546669e+09         2                 False           0  ...     0   \n",
      "\n",
      "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
      "0            0         0      0            0       0        0        1   \n",
      "1            0         0      0            0       0        0        0   \n",
      "2            0         0      0            0       0        0        0   \n",
      "3            0         0      0            0       0        0        0   \n",
      "4            0         0      0            0       0        0        0   \n",
      "\n",
      "   surprise  neutral  \n",
      "0         0        0  \n",
      "1         0        0  \n",
      "2         0        1  \n",
      "3         0        0  \n",
      "4         0        1  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from io import StringIO\n",
    "\n",
    "# URL for the GoEmotions dataset\n",
    "url = \"https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_1.csv\"\n",
    "\n",
    "# Method 1: Using requests to download the file\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    # Create a StringIO object from the response content\n",
    "    csv_data = StringIO(response.text)\n",
    "    \n",
    "    # Read the CSV data into a pandas DataFrame\n",
    "    df_train = pd.read_csv(csv_data)\n",
    "    \n",
    "    # Display basic information about the dataset\n",
    "    print(f\"Dataset shape: {df_train.shape}\")\n",
    "    print(\"\\nFirst 5 rows of the dataset:\")\n",
    "    print(df_train.head())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 37)\n",
      "Index(['text', 'id', 'author', 'subreddit', 'link_id', 'parent_id',\n",
      "       'created_utc', 'rater_id', 'example_very_unclear', 'admiration',\n",
      "       'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',\n",
      "       'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust',\n",
      "       'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy',\n",
      "       'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
      "       'remorse', 'sadness', 'surprise', 'neutral'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "cols = train_df.columns\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1129, 37)\n"
     ]
    }
   ],
   "source": [
    "unclear = train_df[train_df['example_very_unclear']==True]\n",
    "print(unclear.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labels = ['admiration','approval', 'amusement', 'caring', 'desire', \n",
    "              'excitement', 'gratitude', 'joy', 'love','optimism', 'pride', 'relief']\n",
    "neg_labels = ['anger', 'annoyance', 'disappointment', 'disapproval', 'disgust',\n",
    "              'embarrassment','fear', 'grief', 'nervousness', 'remorse', 'sadness']\n",
    "ambi_labels = ['confusion', 'curiosity', 'realization', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_count = {}\n",
    "category_count = {}\n",
    "\n",
    "pos_sum = 0\n",
    "for label in pos_labels:\n",
    "    df = train_df[train_df[label]==1]\n",
    "    emotion_count[label] = len(df)\n",
    "    pos_sum += len(df)\n",
    "category_count['positive'] = pos_sum\n",
    "\n",
    "neg_sum = 0\n",
    "for label in neg_labels:\n",
    "    df = train_df[train_df[label]==1]\n",
    "    emotion_count[label] = len(df)\n",
    "    neg_sum += len(df)\n",
    "category_count['negative'] = neg_sum\n",
    "\n",
    "ambi_sum = 0\n",
    "for label in ambi_labels:\n",
    "    df = train_df[train_df[label]==1]\n",
    "    emotion_count[label] = len(df)\n",
    "    ambi_sum += len(df)\n",
    "\n",
    "df = train_df[train_df['neutral']==1]\n",
    "emotion_count['neutral'] = len(df)\n",
    "ambi_sum += len(df)\n",
    "category_count['neutral'] = ambi_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'admiration': 5647,\n",
       " 'approval': 5928,\n",
       " 'amusement': 3081,\n",
       " 'caring': 1988,\n",
       " 'desire': 1248,\n",
       " 'excitement': 1900,\n",
       " 'gratitude': 3863,\n",
       " 'joy': 2607,\n",
       " 'love': 2745,\n",
       " 'optimism': 2887,\n",
       " 'pride': 452,\n",
       " 'relief': 452,\n",
       " 'anger': 2589,\n",
       " 'annoyance': 4443,\n",
       " 'disappointment': 2771,\n",
       " 'disapproval': 3774,\n",
       " 'disgust': 1704,\n",
       " 'embarrassment': 817,\n",
       " 'fear': 1048,\n",
       " 'grief': 227,\n",
       " 'nervousness': 598,\n",
       " 'remorse': 849,\n",
       " 'sadness': 2193,\n",
       " 'confusion': 2471,\n",
       " 'curiosity': 3267,\n",
       " 'realization': 2867,\n",
       " 'surprise': 1806,\n",
       " 'neutral': 18423}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': 32798, 'negative': 21013, 'neutral': 28834}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger_list = [ \"anger\", \"annoyance\", \"disapproval\", \"disgust\"]\n",
    "fear_list = [\"fear\", \"nervousness\"]\n",
    "joy_list = [\"joy\", \"amusement\", \"approval\", \"excitement\", \"gratitude\",\"love\", \"optimism\", \"relief\", \"pride\", \"admiration\", \"desire\", \"caring\"]\n",
    "sadness_list = [\"sadness\", \"disappointment\", \"embarrassment\", \"grief\", \"remorse\"]\n",
    "surprise_list = [\"surprise\", \"realization\", \"confusion\", \"curiosity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_groups = [anger_list, fear_list, joy_list, sadness_list, surprise_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>group_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That game hurt.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You do right, if you don't care then fuck 'em!</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Man I love reddit.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Right? Considering it‚Äôs such an important docu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text group_label\n",
       "0                                    That game hurt.           3\n",
       "1     You do right, if you don't care then fuck 'em!           5\n",
       "2                                 Man I love reddit.           2\n",
       "3  [NAME] was nowhere near them, he was by the Fa...           5\n",
       "4  Right? Considering it‚Äôs such an important docu...           2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grouping emotions:\n",
    "\n",
    "\"\"\"\n",
    "Labels:\n",
    "Anger (0) : [‚ÄúAnger‚Äù, ‚Äúannoyance‚Äù, ‚Äúdisapproval‚Äù, ‚Äúdisgust‚Äù]\n",
    "Fear (1) : [‚Äúfear‚Äù, ‚Äúnervousness‚Äù ]\n",
    "Joy (2) : [‚Äújoy‚Äù , ‚Äúamusement‚Äù, ‚Äúapproval‚Äù, ‚Äúexcitement‚Äù, ‚Äúgratitude‚Äù,\n",
    "     ‚Äúlove‚Äù, ‚Äúoptimism‚Äù, ‚Äúrelief‚Äù, ‚Äúpride‚Äù, ‚Äúadmiration‚Äù, ‚Äúdesire‚Äù, ‚Äúcaring‚Äù]\n",
    "Sadness (3) : [‚ÄúSadness‚Äù, ‚ÄúDisappointment‚Äù, ‚ÄúEmbarrassment‚Äù, ‚Äúgrief‚Äù, ‚Äúremorse‚Äù]\n",
    "Surprise (4) : [‚ÄúSurprise‚Äù, ‚ÄúRealization‚Äù, ‚Äúconfusion‚Äù, ‚Äúcuriosity‚Äù]\n",
    "Neutral (5) : [\"Neutral\"]\n",
    "\"\"\"\n",
    "\n",
    "col_names = ['text','group_label']\n",
    "\n",
    "new_data = []\n",
    "for id,row in train_df.iterrows():\n",
    "    if row['example_very_unclear'] == True:\n",
    "        continue\n",
    "    else:\n",
    "        if row['neutral'] == True:\n",
    "            data = [row['text'], 5]\n",
    "        else:\n",
    "            max_cnt = -1\n",
    "            max_label = -1\n",
    "            for ix,eg in enumerate(emotion_groups):\n",
    "                cnt = 0\n",
    "                for label in eg:\n",
    "                    if row[label] == 1:\n",
    "                        cnt += 1\n",
    "                if cnt > max_cnt:\n",
    "                    max_cnt = cnt\n",
    "                    max_label = ix\n",
    "            data = [row['text'], max_label]\n",
    "        new_data.append(data)\n",
    "    \n",
    "emotion_group_train = pd.DataFrame(np.array(new_data),columns=col_names)\n",
    "emotion_group_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68871, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_group_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text group_label  \\\n",
      "0                                    That game hurt.           3   \n",
      "1     You do right, if you don't care then fuck 'em!           5   \n",
      "2                                 Man I love reddit.           2   \n",
      "3  [NAME] was nowhere near them, he was by the Fa...           5   \n",
      "4  Right? Considering it‚Äôs such an important docu...           2   \n",
      "\n",
      "   contains_emoticon                                         clean_text  \n",
      "0              False                                          game hurt  \n",
      "1              False                            right dont care fuck em  \n",
      "2              False                                    man love reddit  \n",
      "3              False                           name nowhere near falcon  \n",
      "4              False  right considering it‚Äôs important document know...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# T·∫£i stopwords ti·∫øng Anh (n·∫øu ch∆∞a c√≥)\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Danh s√°ch c√°c t·ª´ d·ª´ng (c√≥ th·ªÉ m·ªü r·ªông n·∫øu c·∫ßn)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# üõ† H√†m x√≥a URL\n",
    "def remove_urls(text):\n",
    "    return re.sub(r\"http.?://[^\\s]+[\\s]?\", \"\", text)\n",
    "\n",
    "# üõ† H√†m x√≥a @mentions\n",
    "def remove_mentions(text):\n",
    "    return re.sub(r\"@\\w+\", \"\", text)\n",
    "\n",
    "# üõ† H√†m x√≥a d·∫•u c√¢u\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "# üõ† H√†m chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng\n",
    "def to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "# üõ† H√†m x√≥a kho·∫£ng tr·∫Øng th·ª´a\n",
    "def remove_whitespaces(text):\n",
    "    return text.strip()\n",
    "\n",
    "# üõ† H√†m x√≥a s·ªë\n",
    "def remove_digits(text):\n",
    "    return re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "# üõ† H√†m x√≥a t·ª´ d·ª´ng\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# üõ† H√†m lo·∫°i b·ªè k√Ω t·ª± l·∫∑p (v√≠ d·ª•: \"coooool\" -> \"cool\")\n",
    "def de_repeat(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")  # B·∫Øt k√Ω t·ª± l·∫∑p t·ª´ 3 l·∫ßn tr·ªü l√™n\n",
    "    return pattern.sub(r\"\\1\\1\", text)  # Thay th·∫ø b·∫±ng ch√≠nh n√≥ nh∆∞ng ch·ªâ l·∫∑p 2 l·∫ßn\n",
    "\n",
    "# üõ† H√†m t·ªïng h·ª£p ƒë·ªÉ l√†m s·∫°ch d·ªØ li·ªáu\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):  # Ki·ªÉm tra n·∫øu gi√° tr·ªã NaN\n",
    "        return \"\"\n",
    "    text = to_lower(text)\n",
    "    text = remove_mentions(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_digits(text)\n",
    "    text = de_repeat(text)\n",
    "    text = remove_whitespaces(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "# üõ† √Åp d·ª•ng v√†o DataFrame `emotion_group_train`\n",
    "# Gi·∫£ s·ª≠ df ƒë√£ ƒë∆∞·ª£c load s·∫µn\n",
    "# df = pd.read_csv(\"emotion_group_train.csv\")  # N·∫øu c·∫ßn ƒë·ªçc t·ª´ file CSV\n",
    "\n",
    "# √Åp d·ª•ng l√†m s·∫°ch d·ªØ li·ªáu\n",
    "emotion_group_train[\"clean_text\"] = emotion_group_train[\"text\"].apply(clean_text)\n",
    "\n",
    "# Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
    "print(emotion_group_train.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text group_label  \\\n",
      "0                                    That game hurt.           3   \n",
      "1     You do right, if you don't care then fuck 'em!           5   \n",
      "2                                 Man I love reddit.           2   \n",
      "3  [NAME] was nowhere near them, he was by the Fa...           5   \n",
      "4  Right? Considering it‚Äôs such an important docu...           2   \n",
      "\n",
      "   contains_emoticon                                         clean_text  \\\n",
      "0              False                                          game hurt   \n",
      "1              False                            right dont care fuck em   \n",
      "2              False                                    man love reddit   \n",
      "3              False                           name nowhere near falcon   \n",
      "4              False  right considering it‚Äôs important document know...   \n",
      "\n",
      "   has_hashtag  has_emoji  has_emoticon  \\\n",
      "0        False      False         False   \n",
      "1        False      False         False   \n",
      "2        False      False         False   \n",
      "3        False      False         False   \n",
      "4        False      False         False   \n",
      "\n",
      "                                     deep_clean_text  \n",
      "0                                          game hurt  \n",
      "1                            right dont care fuck em  \n",
      "2                                    man love reddit  \n",
      "3                           name nowhere near falcon  \n",
      "4  right consider it‚Äôs important document know da...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Danh s√°ch emoticons\n",
    "EMOTICONS = {\n",
    "    \":)\": \"smile\", \":-)\": \"smile\",\n",
    "    \":(\": \"sad\", \":-(\": \"sad\",\n",
    "    \":D\": \"laugh\", \":-D\": \"laugh\",\n",
    "    \";)\": \"wink\", \";-)\": \"wink\",\n",
    "    \":P\": \"playful\", \":-P\": \"playful\"\n",
    "}\n",
    "\n",
    "# T·∫°o regex pattern ƒë·ªÉ t√¨m emoticon\n",
    "emoticon_pattern = r\"|\".join(re.escape(e) for e in EMOTICONS.keys())\n",
    "\n",
    "### 1Ô∏è‚É£ Ki·ªÉm tra n·ªôi dung ƒë·∫∑c bi·ªát ###\n",
    "def contains_hashtag(text):\n",
    "    \"\"\"Ki·ªÉm tra xem vƒÉn b·∫£n c√≥ ch·ª©a hashtag kh√¥ng.\"\"\"\n",
    "    return bool(re.search(r\"#\\s\", text)) if isinstance(text, str) else False\n",
    "\n",
    "def contains_emoji(text):\n",
    "    \"\"\"Ki·ªÉm tra xem vƒÉn b·∫£n c√≥ ch·ª©a emoji kh√¥ng.\"\"\"\n",
    "    return bool(emoji.emoji_count(text)) if isinstance(text, str) else False\n",
    "\n",
    "def contains_emoticon(text):\n",
    "    \"\"\"Ki·ªÉm tra xem vƒÉn b·∫£n c√≥ ch·ª©a emoticon kh√¥ng.\"\"\"\n",
    "    return bool(re.search(emoticon_pattern, text)) if isinstance(text, str) else False\n",
    "\n",
    "### 2Ô∏è‚É£ X·ª≠ l√Ω emoji & emoticon ###\n",
    "def convert_emoji_to_text(text):\n",
    "    \"\"\"Chuy·ªÉn emoji th√†nh t·ª´ m√¥ t·∫£ (vd: üòç -> [heart_eyes])\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    return emoji.demojize(text, delimiters=(\"[\", \"]\"))\n",
    "\n",
    "def convert_emoticon_to_text(text):\n",
    "    \"\"\"Chuy·ªÉn emoticon th√†nh m√¥ t·∫£ (vd: :) -> smile)\"\"\"\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(re.escape(emot), \"_\".join(EMOTICONS[emot].split()), text)\n",
    "    return text\n",
    "\n",
    "\n",
    "### 4Ô∏è‚É£ Stemming & Lemmatization ###\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Chuy·ªÉn ƒë·ªïi POS tag th√†nh d·∫°ng ph√π h·ª£p v·ªõi WordNet\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def stemming(text):\n",
    "    \"\"\"Chuy·ªÉn t·ª´ v·ªÅ d·∫°ng g·ªëc (running -> run, better -> good)\"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    lemma_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "    stemmed_words = [ps.stem(word) for word in lemma_words]\n",
    "    return \" \".join(lemma_words)\n",
    "\n",
    "### 5Ô∏è‚É£ H√†m t·ªïng h·ª£p: Deep Cleaning ###\n",
    "def deep_clean_text(text):\n",
    "    \"\"\"√Åp d·ª•ng t·∫•t c·∫£ c√°c b∆∞·ªõc deep cleaning v√†o vƒÉn b·∫£n\"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = convert_emoji_to_text(text)  # Chuy·ªÉn emoji th√†nh text\n",
    "    text = convert_emoticon_to_text(text)  # Chuy·ªÉn emoticon th√†nh text\n",
    "    text = stemming(text)  # Stemming & Lemmatization\n",
    "    \n",
    "    return text\n",
    "\n",
    "### 6Ô∏è‚É£ √Åp d·ª•ng v√†o DataFrame ###\n",
    "# Gi·∫£ s·ª≠ `emotion_group_train` l√† DataFrame ch·ª©a c·ªôt 'text'\n",
    "# df = pd.read_csv(\"emotion_group_train.csv\")  # N·∫øu c·∫ßn ƒë·ªçc t·ª´ file CSV\n",
    "\n",
    "# T·∫°o c·ªôt ki·ªÉm tra hashtag, emoji, emoticon\n",
    "emotion_group_train[\"has_hashtag\"] = emotion_group_train[\"clean_text\"].apply(contains_hashtag)\n",
    "emotion_group_train[\"has_emoji\"] = emotion_group_train[\"clean_text\"].apply(contains_emoji)\n",
    "emotion_group_train[\"has_emoticon\"] = emotion_group_train[\"clean_text\"].apply(contains_emoticon)\n",
    "\n",
    "# √Åp d·ª•ng Deep Cleaning\n",
    "emotion_group_train[\"deep_clean_text\"] = emotion_group_train[\"clean_text\"].apply(deep_clean_text)\n",
    "\n",
    "# Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
    "print(emotion_group_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
