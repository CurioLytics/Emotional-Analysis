{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (70000, 37)\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "                                                text       id  \\\n",
      "0                                    That game hurt.  eew5j0j   \n",
      "1   >sexuality shouldn’t be a grouping category I...  eemcysk   \n",
      "2     You do right, if you don't care then fuck 'em!  ed2mah1   \n",
      "3                                 Man I love reddit.  eeibobj   \n",
      "4  [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
      "\n",
      "                author            subreddit    link_id   parent_id  \\\n",
      "0                Brdd9                  nrl  t3_ajis4z  t1_eew18eq   \n",
      "1          TheGreen888     unpopularopinion  t3_ai4q37   t3_ai4q37   \n",
      "2             Labalool          confessions  t3_abru74  t1_ed2m7g7   \n",
      "3        MrsRobertshaw             facepalm  t3_ahulml   t3_ahulml   \n",
      "4  American_Fascist713  starwarsspeculation  t3_ackt2f  t1_eda65q2   \n",
      "\n",
      "    created_utc  rater_id  example_very_unclear  admiration  ...  love  \\\n",
      "0  1.548381e+09         1                 False           0  ...     0   \n",
      "1  1.548084e+09        37                  True           0  ...     0   \n",
      "2  1.546428e+09        37                 False           0  ...     0   \n",
      "3  1.547965e+09        18                 False           0  ...     1   \n",
      "4  1.546669e+09         2                 False           0  ...     0   \n",
      "\n",
      "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
      "0            0         0      0            0       0        0        1   \n",
      "1            0         0      0            0       0        0        0   \n",
      "2            0         0      0            0       0        0        0   \n",
      "3            0         0      0            0       0        0        0   \n",
      "4            0         0      0            0       0        0        0   \n",
      "\n",
      "   surprise  neutral  \n",
      "0         0        0  \n",
      "1         0        0  \n",
      "2         0        1  \n",
      "3         0        0  \n",
      "4         0        1  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from io import StringIO\n",
    "\n",
    "# URL for the GoEmotions dataset\n",
    "url = \"https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_1.csv\"\n",
    "\n",
    "# Method 1: Using requests to download the file\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    # Create a StringIO object from the response content\n",
    "    csv_data = StringIO(response.text)\n",
    "    \n",
    "    # Read the CSV data into a pandas DataFrame\n",
    "    df_train = pd.read_csv(csv_data)\n",
    "    \n",
    "    # Display basic information about the dataset\n",
    "    print(f\"Dataset shape: {df_train.shape}\")\n",
    "    print(\"\\nFirst 5 rows of the dataset:\")\n",
    "    print(df_train.head())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 37)\n",
      "Index(['text', 'id', 'author', 'subreddit', 'link_id', 'parent_id',\n",
      "       'created_utc', 'rater_id', 'example_very_unclear', 'admiration',\n",
      "       'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',\n",
      "       'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust',\n",
      "       'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy',\n",
      "       'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
      "       'remorse', 'sadness', 'surprise', 'neutral'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "cols = train_df.columns\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1129, 37)\n"
     ]
    }
   ],
   "source": [
    "unclear = train_df[train_df['example_very_unclear']==True]\n",
    "print(unclear.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labels = ['admiration','approval', 'amusement', 'caring', 'desire', \n",
    "              'excitement', 'gratitude', 'joy', 'love','optimism', 'pride', 'relief']\n",
    "neg_labels = ['anger', 'annoyance', 'disappointment', 'disapproval', 'disgust',\n",
    "              'embarrassment','fear', 'grief', 'nervousness', 'remorse', 'sadness']\n",
    "ambi_labels = ['confusion', 'curiosity', 'realization', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_count = {}\n",
    "category_count = {}\n",
    "\n",
    "pos_sum = 0\n",
    "for label in pos_labels:\n",
    "    df = train_df[train_df[label]==1]\n",
    "    emotion_count[label] = len(df)\n",
    "    pos_sum += len(df)\n",
    "category_count['positive'] = pos_sum\n",
    "\n",
    "neg_sum = 0\n",
    "for label in neg_labels:\n",
    "    df = train_df[train_df[label]==1]\n",
    "    emotion_count[label] = len(df)\n",
    "    neg_sum += len(df)\n",
    "category_count['negative'] = neg_sum\n",
    "\n",
    "ambi_sum = 0\n",
    "for label in ambi_labels:\n",
    "    df = train_df[train_df[label]==1]\n",
    "    emotion_count[label] = len(df)\n",
    "    ambi_sum += len(df)\n",
    "\n",
    "df = train_df[train_df['neutral']==1]\n",
    "emotion_count['neutral'] = len(df)\n",
    "ambi_sum += len(df)\n",
    "category_count['neutral'] = ambi_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'admiration': 5647,\n",
       " 'approval': 5928,\n",
       " 'amusement': 3081,\n",
       " 'caring': 1988,\n",
       " 'desire': 1248,\n",
       " 'excitement': 1900,\n",
       " 'gratitude': 3863,\n",
       " 'joy': 2607,\n",
       " 'love': 2745,\n",
       " 'optimism': 2887,\n",
       " 'pride': 452,\n",
       " 'relief': 452,\n",
       " 'anger': 2589,\n",
       " 'annoyance': 4443,\n",
       " 'disappointment': 2771,\n",
       " 'disapproval': 3774,\n",
       " 'disgust': 1704,\n",
       " 'embarrassment': 817,\n",
       " 'fear': 1048,\n",
       " 'grief': 227,\n",
       " 'nervousness': 598,\n",
       " 'remorse': 849,\n",
       " 'sadness': 2193,\n",
       " 'confusion': 2471,\n",
       " 'curiosity': 3267,\n",
       " 'realization': 2867,\n",
       " 'surprise': 1806,\n",
       " 'neutral': 18423}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': 32798, 'negative': 21013, 'neutral': 28834}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger_list = [ \"anger\", \"annoyance\", \"disapproval\", \"disgust\"]\n",
    "fear_list = [\"fear\", \"nervousness\"]\n",
    "joy_list = [\"joy\", \"amusement\", \"approval\", \"excitement\", \"gratitude\",\"love\", \"optimism\", \"relief\", \"pride\", \"admiration\", \"desire\", \"caring\"]\n",
    "sadness_list = [\"sadness\", \"disappointment\", \"embarrassment\", \"grief\", \"remorse\"]\n",
    "surprise_list = [\"surprise\", \"realization\", \"confusion\", \"curiosity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_groups = [anger_list, fear_list, joy_list, sadness_list, surprise_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>group_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That game hurt.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You do right, if you don't care then fuck 'em!</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Man I love reddit.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Right? Considering it’s such an important docu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text group_label\n",
       "0                                    That game hurt.           3\n",
       "1     You do right, if you don't care then fuck 'em!           5\n",
       "2                                 Man I love reddit.           2\n",
       "3  [NAME] was nowhere near them, he was by the Fa...           5\n",
       "4  Right? Considering it’s such an important docu...           2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grouping emotions:\n",
    "\n",
    "\"\"\"\n",
    "Labels:\n",
    "Anger (0) : [“Anger”, “annoyance”, “disapproval”, “disgust”]\n",
    "Fear (1) : [“fear”, “nervousness” ]\n",
    "Joy (2) : [“joy” , “amusement”, “approval”, “excitement”, “gratitude”,\n",
    "     “love”, “optimism”, “relief”, “pride”, “admiration”, “desire”, “caring”]\n",
    "Sadness (3) : [“Sadness”, “Disappointment”, “Embarrassment”, “grief”, “remorse”]\n",
    "Surprise (4) : [“Surprise”, “Realization”, “confusion”, “curiosity”]\n",
    "Neutral (5) : [\"Neutral\"]\n",
    "\"\"\"\n",
    "\n",
    "col_names = ['text','group_label']\n",
    "\n",
    "new_data = []\n",
    "for id,row in train_df.iterrows():\n",
    "    if row['example_very_unclear'] == True:\n",
    "        continue\n",
    "    else:\n",
    "        if row['neutral'] == True:\n",
    "            data = [row['text'], 5]\n",
    "        else:\n",
    "            max_cnt = -1\n",
    "            max_label = -1\n",
    "            for ix,eg in enumerate(emotion_groups):\n",
    "                cnt = 0\n",
    "                for label in eg:\n",
    "                    if row[label] == 1:\n",
    "                        cnt += 1\n",
    "                if cnt > max_cnt:\n",
    "                    max_cnt = cnt\n",
    "                    max_label = ix\n",
    "            data = [row['text'], max_label]\n",
    "        new_data.append(data)\n",
    "    \n",
    "emotion_group_train = pd.DataFrame(np.array(new_data),columns=col_names)\n",
    "emotion_group_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68871, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_group_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text group_label  \\\n",
      "0                                    That game hurt.           3   \n",
      "1     You do right, if you don't care then fuck 'em!           5   \n",
      "2                                 Man I love reddit.           2   \n",
      "3  [NAME] was nowhere near them, he was by the Fa...           5   \n",
      "4  Right? Considering it’s such an important docu...           2   \n",
      "\n",
      "   contains_emoticon                                         clean_text  \n",
      "0              False                                          game hurt  \n",
      "1              False                            right dont care fuck em  \n",
      "2              False                                    man love reddit  \n",
      "3              False                           name nowhere near falcon  \n",
      "4              False  right considering it’s important document know...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Tải stopwords tiếng Anh (nếu chưa có)\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Danh sách các từ dừng (có thể mở rộng nếu cần)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# 🛠 Hàm xóa URL\n",
    "def remove_urls(text):\n",
    "    return re.sub(r\"http.?://[^\\s]+[\\s]?\", \"\", text)\n",
    "\n",
    "# 🛠 Hàm xóa @mentions\n",
    "def remove_mentions(text):\n",
    "    return re.sub(r\"@\\w+\", \"\", text)\n",
    "\n",
    "# 🛠 Hàm xóa dấu câu\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "# 🛠 Hàm chuyển thành chữ thường\n",
    "def to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "# 🛠 Hàm xóa khoảng trắng thừa\n",
    "def remove_whitespaces(text):\n",
    "    return text.strip()\n",
    "\n",
    "# 🛠 Hàm xóa số\n",
    "def remove_digits(text):\n",
    "    return re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "# 🛠 Hàm xóa từ dừng\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# 🛠 Hàm loại bỏ ký tự lặp (ví dụ: \"coooool\" -> \"cool\")\n",
    "def de_repeat(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")  # Bắt ký tự lặp từ 3 lần trở lên\n",
    "    return pattern.sub(r\"\\1\\1\", text)  # Thay thế bằng chính nó nhưng chỉ lặp 2 lần\n",
    "\n",
    "# 🛠 Hàm tổng hợp để làm sạch dữ liệu\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):  # Kiểm tra nếu giá trị NaN\n",
    "        return \"\"\n",
    "    text = to_lower(text)\n",
    "    text = remove_mentions(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_digits(text)\n",
    "    text = de_repeat(text)\n",
    "    text = remove_whitespaces(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "# 🛠 Áp dụng vào DataFrame `emotion_group_train`\n",
    "# Giả sử df đã được load sẵn\n",
    "# df = pd.read_csv(\"emotion_group_train.csv\")  # Nếu cần đọc từ file CSV\n",
    "\n",
    "# Áp dụng làm sạch dữ liệu\n",
    "emotion_group_train[\"clean_text\"] = emotion_group_train[\"text\"].apply(clean_text)\n",
    "\n",
    "# Hiển thị kết quả\n",
    "print(emotion_group_train.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text group_label  \\\n",
      "0                                    That game hurt.           3   \n",
      "1     You do right, if you don't care then fuck 'em!           5   \n",
      "2                                 Man I love reddit.           2   \n",
      "3  [NAME] was nowhere near them, he was by the Fa...           5   \n",
      "4  Right? Considering it’s such an important docu...           2   \n",
      "\n",
      "   contains_emoticon                                         clean_text  \\\n",
      "0              False                                          game hurt   \n",
      "1              False                            right dont care fuck em   \n",
      "2              False                                    man love reddit   \n",
      "3              False                           name nowhere near falcon   \n",
      "4              False  right considering it’s important document know...   \n",
      "\n",
      "   has_hashtag  has_emoji  has_emoticon  \\\n",
      "0        False      False         False   \n",
      "1        False      False         False   \n",
      "2        False      False         False   \n",
      "3        False      False         False   \n",
      "4        False      False         False   \n",
      "\n",
      "                                     deep_clean_text  \n",
      "0                                          game hurt  \n",
      "1                            right dont care fuck em  \n",
      "2                                    man love reddit  \n",
      "3                           name nowhere near falcon  \n",
      "4  right consider it’s important document know da...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Danh sách emoticons\n",
    "EMOTICONS = {\n",
    "    \":)\": \"smile\", \":-)\": \"smile\",\n",
    "    \":(\": \"sad\", \":-(\": \"sad\",\n",
    "    \":D\": \"laugh\", \":-D\": \"laugh\",\n",
    "    \";)\": \"wink\", \";-)\": \"wink\",\n",
    "    \":P\": \"playful\", \":-P\": \"playful\"\n",
    "}\n",
    "\n",
    "# Tạo regex pattern để tìm emoticon\n",
    "emoticon_pattern = r\"|\".join(re.escape(e) for e in EMOTICONS.keys())\n",
    "\n",
    "### 1️⃣ Kiểm tra nội dung đặc biệt ###\n",
    "def contains_hashtag(text):\n",
    "    \"\"\"Kiểm tra xem văn bản có chứa hashtag không.\"\"\"\n",
    "    return bool(re.search(r\"#\\s\", text)) if isinstance(text, str) else False\n",
    "\n",
    "def contains_emoji(text):\n",
    "    \"\"\"Kiểm tra xem văn bản có chứa emoji không.\"\"\"\n",
    "    return bool(emoji.emoji_count(text)) if isinstance(text, str) else False\n",
    "\n",
    "def contains_emoticon(text):\n",
    "    \"\"\"Kiểm tra xem văn bản có chứa emoticon không.\"\"\"\n",
    "    return bool(re.search(emoticon_pattern, text)) if isinstance(text, str) else False\n",
    "\n",
    "### 2️⃣ Xử lý emoji & emoticon ###\n",
    "def convert_emoji_to_text(text):\n",
    "    \"\"\"Chuyển emoji thành từ mô tả (vd: 😍 -> [heart_eyes])\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    return emoji.demojize(text, delimiters=(\"[\", \"]\"))\n",
    "\n",
    "def convert_emoticon_to_text(text):\n",
    "    \"\"\"Chuyển emoticon thành mô tả (vd: :) -> smile)\"\"\"\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(re.escape(emot), \"_\".join(EMOTICONS[emot].split()), text)\n",
    "    return text\n",
    "\n",
    "\n",
    "### 4️⃣ Stemming & Lemmatization ###\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Chuyển đổi POS tag thành dạng phù hợp với WordNet\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def stemming(text):\n",
    "    \"\"\"Chuyển từ về dạng gốc (running -> run, better -> good)\"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    lemma_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "    stemmed_words = [ps.stem(word) for word in lemma_words]\n",
    "    return \" \".join(lemma_words)\n",
    "\n",
    "### 5️⃣ Hàm tổng hợp: Deep Cleaning ###\n",
    "def deep_clean_text(text):\n",
    "    \"\"\"Áp dụng tất cả các bước deep cleaning vào văn bản\"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = convert_emoji_to_text(text)  # Chuyển emoji thành text\n",
    "    text = convert_emoticon_to_text(text)  # Chuyển emoticon thành text\n",
    "    text = stemming(text)  # Stemming & Lemmatization\n",
    "    \n",
    "    return text\n",
    "\n",
    "### 6️⃣ Áp dụng vào DataFrame ###\n",
    "# Giả sử `emotion_group_train` là DataFrame chứa cột 'text'\n",
    "# df = pd.read_csv(\"emotion_group_train.csv\")  # Nếu cần đọc từ file CSV\n",
    "\n",
    "# Tạo cột kiểm tra hashtag, emoji, emoticon\n",
    "emotion_group_train[\"has_hashtag\"] = emotion_group_train[\"clean_text\"].apply(contains_hashtag)\n",
    "emotion_group_train[\"has_emoji\"] = emotion_group_train[\"clean_text\"].apply(contains_emoji)\n",
    "emotion_group_train[\"has_emoticon\"] = emotion_group_train[\"clean_text\"].apply(contains_emoticon)\n",
    "\n",
    "# Áp dụng Deep Cleaning\n",
    "emotion_group_train[\"deep_clean_text\"] = emotion_group_train[\"clean_text\"].apply(deep_clean_text)\n",
    "\n",
    "# Hiển thị kết quả\n",
    "print(emotion_group_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
